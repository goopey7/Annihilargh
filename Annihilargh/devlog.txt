Wednesday (2/3/21)
After watching quite a few C++ and WINAPI tutorials. I finally decided to start writing some code in class today.
I have learned/watched videos that should cover pretty much everything I need to implement before getting into DirectX.
I have also learned of some things that DirectX does use such as COM objects and how to use interfaces with them.

Anyways, did a ton of stuff today. I created a really cool wrapper class for creating Windows which nicely encapsulates
all of the nasty WINAPI stuff. I also have implemented a very verbose way to throw exceptions using the WINAPI 
and I have implemented some error checking in my code. I'm not going to error check the stuff that happens before the 
creation of a window because I've never had it fail, and it would be too much time and effort.

Tomorrow or Friday I plan to create a Keyboard class which will act as an interface between the Window class and the 
game engine. The message handler in the Window class deals with input, so we need to give that to the keyboard and
then the game will get key events from the keyboard. Nice encapsulation away from the WINAPI.

Friday (2/5/21)
Today I created the keyboard class which acts as an interface between the Window class and the rest of the engine.
Next we have to deal with the mouse.

Saturday (2/6/21)
Got a nice mouse class similar to the keyboard class. Also got mouse capturing working. So, if the user drags off 
the client region and doesn't release a mouse button, move events still occur until the mouse buttons are released.
Also dealt with wheel delta. So now all mice are treated equally. So one mouse doesn't scroll faster or slower
than the other.

Tuesday (2/9/21)
Added a class to deal with game logic.
Also, moved the message processing in Annihilargh.cpp over to the Window class

// Yeah I know this isn't grammatically correct but it gets the info across.
Currently learning a bit about COM (Component Object Model) which DirectX utilizes.
Fortunately Direct3D will do a lot of this for me, so I don't have to become an expert in COM.
Basically COM gives us binary compatibility. Provides us a nice stable interface.
COM is language agnostic. It gives compatibility between different programming languages as well since
it is binary. Has it's own way of resource allocation since it is language independent.
Every COM object has a unique ID. COM objects are interacted with using an interface.
So if the way COM objects work or get created ever get changed via an update or something. The code will still work
thanks to the interface. COM uses reference counting, so we never explicitly have to delete it. Instead we call
Release which will decrement the reference count. We don't use 'new' either since that is language specific.
We use some kind of function to create a COM object. You call AddRef when you want to add something that works with
a COM object. QueryInterface takes a uuid (Universal Unique Identifier) of a COM object and a ptr to a ptr of the 
desired interface. It then checks to see if the COM object actually supports the interface provided, and
if it does, fills the ptr with a ptr to the queried interface. If QueryInterface is successful it will also
increase the reference count on the COM object since there is now another interface pointing to it.

Ok now I'm going to try and explain a bit about the Direct3D architecture.
We start with the Device which represents the graphics card/adapter.
The Device object is used to create all the other objects.

A SwapChain contains two frame buffers. The front buffer only contains a finished frame, and is what gets sent to the
screen. The back buffer is what we draw onto. We then swap the front and back buffers and repeat the process.
The reason we do this is because it would look awful if the monitor was reading row by row of our frame buffer while
we are also drawing onto it at the same time. What if we aren't done drawing an object that the screen has already read?
That's why we use this double buffer system.

However, screen tearing is still a possible issue. If the framebuffers swap while the monitor is in the middle of 
reading a frame buffer, there will be a tear in the screen (not literally). So we must make sure we only swap buffers
once the screen is done drawing. That's what Vsync is.

Device is used for allocating resources and creating objects. The Device Context is used for actually drawing and 
issuing rendering commands. 

Sunday (2/14/21)
Implemented Error handling for graphics w/directx functions
Also changed all dumb COM ptrs to smart COM ptrs.

Drew first triangle using the Direct3D pipeline. Currently very sloppy and inefficient since we are having to allocate
everything in the pipeline every draw call, but the entire pipeline is easy to see in one function.

Tuesday (2/16/21)
Now that I have a quick and dirty triangle drawing function, I am going to mess around with the pipeline.
See what I can do, and I'll probably also break stuff.

Adding triangles is as simple as just adding three more vertices to the list of vertices.
One thing I have found is that triangles are not displayed if the vertices wind counter clockwise.
Meaning we have to draw vertices in a clockwise manner. (top then right then left)

If I want to draw lines instead of triangles, all I have to do is change the primitive topology.
pDeviceContext->IASetPrimitiveTopology(D3D11_PRIMITIVE_TOPOLOGY_LINELIST);
Obviously lines are different from triangles. They require two vertices not three. But nothing bad
happens if the amount of vertices is not a multiple of two.

I have assigned each vertex of the triangle a different colour. The rasterizer blends them together in a cool way.

Friday (2/19/21)
Implemented indexed drawing. This allows us to compress the vertex buffer because we don't have to fill the buffer
with duplicate vertices. Instead, we have an index buffer that indexes into the vertex buffer so we can reuse the same
vertex without duplicating it in the vertex buffer!
This reduces the amount of vertex data that is sent to the graphics adapter, and also gives the chance for the graphics
adapter to fetch a recently used vertex from its vertex cache which is better for performance.

Monday (2/22/21)
Unfortunately I did not work on this over the weekend. I left my literature review for senior plan for that weekend,
so I mainly did that. I also have no free periods. Meaning I am in 9 classes including senior plan.

Anyways, had a big session today. Today I accomplished drawing a 3D cube that rotates. I also managed to colour in all
the sides an individual colour. Each side is made up of two triangles. The triangles are drawn using indexed drawing.
So we have a vertex buffer containing the vertices, and an index buffer defining how the vertices are connected, so
the actual triangles that are drawn. To actually colour the faces, I setup a constant buffer containing 6 colours,
one for each face, for the pixel shader. Then in the pixel shader you can get Direct3D to assign an index all the 
triangles, and then I used that index to select a colour from the constant buffer that was just bound to the shader.

Also added a z-buffer. This allows us to properly simulate depth. Before adding the z-buffer, objects would be drawn one
on top of the other just depending on the order they were drawn. So, we create a depth texture by comparing the z-value
of all vertices or maybe pixels hmmmmmm. Smaller z-values are closer to the screen, so they should be drawn last. That's
the idea, and it works now.

Tuesday (2/23/21)
Yesterday was a great success! But now it is time to do some design because right now the entire pipeline is in one 
function. All bindable elements of the pipeline and the buffers used to represent the cube, are created and destroyed
every frame. The function was great for learning about the pipeline, but now that I do understand how everything works,
it is time to refactor. This current function is definitely not going to work in the future because the framerate is
going to suffer a lot for no good reason if we spawn in a thousand cubes every frame. So, I want these buffers to
persist between frames.

So, here's what the design is going to look like so everything is nice, neat, and convenient:

Create a Bindable interface containing a Bind function that gets implemented by anything that needs to be bound to the 
pipeline. Such as our shaders, constant buffer etc. The Bind function will take in a Graphics object as a parameter.
So we will create classes for each bindable thing, and they will inherit from the bindable interface.

Also create a Drawable interface which can be implemented by things such as a Cube object. Drawable objects will contain
bindable objects. Definitely a vertex shader and a pixel shader for example. However, objects that implement Drawable
will have different sets of Bindable objects. So, what's nice about this is that these varying Bindables can just be
put into a std::vector using polymorphism. So Drawable supports any combination of Bindable objects.
Drawable will have an already implemented Draw function which takes in a Graphics object as a parameter.
This function is simply going to bind all bindables to the pipeline using the Bind function. Then it will
call Draw through the graphics object passed in. 

This design also provides persistence of bindables between frames since they are all going to be their own object.
So not only does the code look good, but it actually also performs better too.

Now I have to actually implement this ingenious design plan.

A few hours later... (doing other work and dinner)

(22:45) Let's start implementing this plan!

Wednesday (2/24/21)

(2:16AM) well I finally did it. spent 2-3 hrs debugging. I think I probably should have stopped earlier I'm definitely
too tired. I was copy/pasting from the DrawTestTriangle function in Graphics and so I missed a few things.
I really think I should probably go to bed. I think I have a calc BC quiz tomorrow lol. Well now I can spawn in 
loads of cubes with ease!!!

Saturday (2/27/21)
The Bindable/Drawable design works now, but it can still be improved. First thing I'd like to improve is the fact that
all 100 cubes being spawned use the exact same D3D elements, but each cube instance has their own instances of D3D 
elements. So, we're creating 100 vertex buffers, 100 pixel shaders etc.
That's pretty wasteful and so we want every drawable object to access a list of its common bindables shared with other
drawables of the same type.
We can get around that with the use of static variables. However, it would kind of suck if I had to copy/paste this for 
every type of drawable. Also, the Draw function is only aware of the per instance bindables in the vector, 
not static ones pertaining to a specific type of Drawable.
And I can't do it in the Drawable interface because that would create one static vector shared between
all types that inherit from Drawable. We don't want that. We want each Drawable type to have its own shared vector.
So for example all Cubes share a vector of Bindables and all spheres share a different vector of Bindables. 
Fortunately we can get the compiler to do the copy/pasting for us by creating a DrawableBase<T> class which is
templated. This class would sit in between the drawable types and the drawable interface.
So our cube would inherit from DrawableBase<Cube> and DrawableBase<T> inherits from Drawable.
That way the compiler will copy/paste the DrawableBase<T> class for every different template used. Meaning we get
different static vectors for different types. And we can just use the type of Drawable for the template because it 
works and it's intuitive.

So now that I have implemented that, the only Bindable that is not static for cubes is the TransformationConstantBuffer
because all the different cube instances are going to have different positions and rotations. However, the entire 
TransformationConstantBuffer doesn't need to be separate for each cube. All that really needs to happen to transform a
cube is to update a constant buffer and then bind it to the pipeline. And it turns out that we can reuse the same 
constant buffer for each cube and then each cube would have to update it with various transformations.
This is an easy fix. Just simply make a static ptr to a vertex constant buffer. That way the same vertex constant buffer
persists between different instances of TransformationConstantBuffer. So I have severely decreased the amount of
resources being duplicated for no reason, and I am noticing an increase in performance when running the program now.

To properly test to make sure this works I should definitely add other Drawables than just the Cube.

Monday (3/1/21)
Separated the Mathematical representation of a Cube from the Drawable Cube that binds to the pipeline.
I'm going to have some Geometric Primitives that can be created statically. Next I'm going to try and implement
a sphere which is gonna be difficult.

Well I got the sphere kind of working. But I can see that the triangles aren't being drawn correctly.
Geez that was a lot of math.
Anyways, I'm looking forward to loading 3D models so I can forget about 3D geometry :)
Ok, I fixed the indexes so now the triangles have disappeared and you see a sphere.
But the pixel shader was not colouring in all the triangles hmmmmmm.
Well after 30 minutes turns out my shaders were compiling to a different directory, meaning that the changes I made
to the pixel shader were not actually affecting my program at all. Lol. Now it all looks good. 

I also tested out how nice it would be to use this system in Game.cpp. It's beautiful. Just have a list of drawables
containing cubes and melons. Ahhhh beautiful.

Tuesday (3/2/21)
Time to start learning about how to implement textures in Direct3D!
Instead of using the helper functions in the DirectXTK. I'm gonna load the images using GDIPlus and then manually
get them into the Direct3D pipeline.

Got GDIPlus working. Now I need to get Textures in Direct3D working. You bind textures to the pipeline, but you also
need a sampler which specifies how the texture is going to be used. Is it going to be wrapped how will it be filtered?
Linear interpolation? Point sampling? Anisotropic? Depends on what the art style is.

Well I've done it. Fortunately for more complex meshes loaded from files, the texture coordinates are already done for
you.

Now I'm going to incorporate ImGui (https://github.com/ocornut/imgui) into the project. I'm not making my own GUI system
because life is too short.

Got imgui integrated into my code. Everything is looking pretty good.
Also can now control the camera using a camera class and we have a GUI for controlling the camera's location and 
rotation.

Sunday (3/14/21)
Yeah took a massive break whoops. Had a lot of things to do over the last week, and knowing that I can put in a lot of
hours over spring break put me off working on it last week. Now I have much more uninterrupted time to spend with this
project. So, today I'm going to try and implement some form of dynamic lighting. We're going to be doing per-pixel
lighting using point lights, and specular highlighting. This is like Phong shading, and we touched on this a little bit
in the ray tracing project last year. Basically the pixels closer to the point light should be brighter than the pixels
further from the point light. Angle also determines attenuation, 90 degrees, or pi/2 is going to be brightest. So, every
pixel will calculate it's colour relative to the light sources, and the normals of these pixels are calculated by
interpolating between two vertices.

Monday (3/15/21)
So I got some kind of lighting, but it's ugly and super unrealistic. Basically the triangles that are lit
turn white, while the unlit triangles remain a dark grey colour. It's a great step in the right direction but it needs
more work. Also as I move the point light around, it doesn't seem to have any effect on the lighting of the cubes.
Interesting. The culprit is most likely the pixel shader since that's where the colour of the pixels are determined.
So it's time to bring out Visual Studio's graphics debugger. It's really cool. It attaches to the program, and allows me
to capture frames. I can then exit the program, and open up a captured frame. Then if I click on a pixel, I can see
everything that went into determining the pixel's colour. I can then run a debugger on either the vertex shader or the
pixel shader for the pixel I selected, and see what's going on. So cool.

So turns out the issue was a physics error. No wonder the light was always blown out. As you get further and further
from the light, the light on the surface should be dimmer. So instead of just multiplying by attenuation I should be
dividing. As the relationship is inverse. I also was just getting the length of the wrong vector resulting in everything
being off. Now that I have fixed both those problems, I don't seem to be getting much lighting at all. When I check the
debugger, my diffuse colours are less than 0.02 in all channels. That's not very good either. That just means that the
attenuation is too high. Right now the coefficients are all set to 1. They should be smaller.
This website gives some coefficients for different luminosity http://wiki.ogre3d.org/-Point+Light+Attenuation
So I just picked the coefficients for 50 since I want a dim light to really see this working. I also made the background
and ambient colour for the pixel shader much darker. Sure enough this works. I've messed around with the luminosity and
stuck with 160. However, when I move the point light, the lighting on the cubes still does not change.
Looks like the constant buffer in the pixel shader that contains the light position doesn't change when the light
changes location. Ah ha, and the constant buffer is being bound by the lit cubes instead of the global point light.
Got rid of that, and still the problem isn't solved. Now the constant buffer contains 1,1,1, and a garbage value.

Tuesday (3/16/21)
Now the debugger is saying that the constant buffer was set by a previous frame. Which must mean that now the constant
buffer is only being bound at the start, and not getting updated every frame. Also not sure why the data is 1,1,1.
Ahhhh for some reason the colour of the solid sphere is getting bound to the phong pixel shader. Meaning that we take
in the colour white, and say that's where the light is. Because the solid sphere, the physical embodiment of the point
light, is white. For some reason that constant buffer is getting bound, and the light position buffer is not getting
bound at all. And sure enough, after calling bind on our constant buffer in the Bind function in PointLight. The
lighting now does dynamically change when moving the point light. Hooray!

Now I'm going to get rid of all of the static constants declared in the pixel shader, and put them in the constant
buffer so the data can be stored CPU side in the PointLight class. Ran into a funny issue after doing this though.
All of my cubes turned neon green. Got no errors, the program ran fine. However, when I checked the debugger output,
I was getting loads of warnings saying that the size of the constant buffer being passed to the pixel shader was too
small. Fortunately I've dealt with this problem before. It's because the shader is expecting padding so that the float3
values are aligned in 16 byte chunks. However I wasn't doing that, so the shader was getting float3's lodged together
each taking 12 bytes, and so the pixel shader was not reading the float3's correctly. I could insert a float after every
float3 in the data structure in PointLight, but there is a cooler way. I can use alignas(16) on the left side of my
float3's and the compiler will do the padding for me. How nice! 

So now that the light constants have been moved into the constant buffer, we can now change them every frame. So I'll
quickly add some customisation for every constant in the ImGui panel for the PointLight. Ok that now works and looks
good.

Next step is to fix the fact that the material colour is being stored in the PointLight, when really it should be a
property of the drawable object. So we'll need to add another constant buffer to the phong pixel shader, which means
that we also need to change the ConstantBuffer class so that it can deal with slots, because right now it defaults to
slot 0. But our object material constant buffer has to be bound to slot 1.

Implemented that with no issues, and to test our new constant buffer, I randomly assigned a colour to each cube.
And indeed they all are dynamically lit with different colours.

Now that our drawable objects can have their own colour. Let's add to these material properties and add data for
specular highlights.

Got specular highlights working. The scene is looking quite nice right now.

In anticipation of adding more drawable objects than just our Cube, I'm going to add another layer of inheritance which
will deal with all the basic things future objects will likely have such as position, rotation, and scale.

Wednesday (3/17/21)
Couldn't sleep last night for some strange reason. Just lied awake until I decided to get on with it at 7AM.
I'm on some random timezone now. Lovely. Let's see how much progress I can make before the adenosine hits the brain.
Next order of business is to load in 3D meshes from files because doing all of this geometry is exhausting, and it will
allow us to use complex 3D models. To load in models I'm going to make use of this library:
https://github.com/assimp/assimp
It supports many different file formats, and it's popular. So it looks good. Also seems very easy to load models into
Direct3D. Basically just going to be breaking the model into triangles. So any polygons with more than 3 sides are
turned into triangles. Then loop through all the triangles and add them to the vertex buffer.

Friday (3/19/21)
I have built assimp and added it into the project. I have also dropped support for x86 architectures because it is going
to be pretty inconvenient to generate binaries for both 64 bit and 32 bit versions. 